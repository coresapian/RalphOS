# RalphOS Progress & Learnings

This file captures accumulated knowledge from Ralph iterations. Each entry documents implementation details, patterns discovered, and lessons learned.

---

## Codebase Patterns (Reusable Knowledge)

### URL Discovery
- Always check for pagination (numeric pages, "load more" buttons, infinite scroll)
- Normalize URLs: remove fragments (#), trailing slashes, and duplicates
- Save intermediate results every 50 URLs to prevent data loss
- Use set() for deduplication before writing to urls.json

### HTML Scraping
- Always set User-Agent header to avoid basic bot detection
- Implement retry logic with exponential backoff (1s, 2s, 4s delays)
- Save HTML to `{output_dir}/html/{slug}.html` format
- Use filename sanitization: replace special chars, limit to 200 chars

### Build Extraction
- Use schema at `schema/build_extraction_schema.json` as reference
- Required fields: build_id, source_type, build_type, build_source, source_url, year, make, model
- Generate build_id with: `python scripts/tools/build_id_generator.py {source_url}`
- Set extraction_confidence based on completeness of extracted data

### Mod Extraction
- Use `category_detector.py` for automatic categorization
- Categories come from `schema/Vehicle_Componets.json`
- Include brand extraction when identifiable from mod name
- Calculate modification_level: Stock (0-1), Light (2-5), Moderate (6-15), Heavy (16+)

### Anti-Bot Handling
- If 403/429 errors occur, mark source as "blocked" in sources.json
- Use stealth_scraper.py for blocked sources
- Rate limit: minimum 1 second between requests per source

### File Structure
- urls.json: `{"urls": [], "lastUpdated": "ISO8601", "totalCount": N}`
- builds.json: `{"builds": [...], "metadata": {...}}`
- mods.json: `{"mods": [...], "metadata": {...}}`
- scrape_progress.json: tracks completed/failed URLs

---

## Session History

### 2026-01-08 - Initial Setup
- Created progress.txt template
- Documented codebase patterns for URL discovery, scraping, extraction
- Added anti-bot handling guidelines
- Established file structure conventions

---

### 2026-01-08 - ECD Auto Design URL Discovery (Stage 1)
- **Source**: ECD Auto Design (ecdautodesign.com/ecd-showcase/)
- **Action**: URL Discovery for Stage 1 (SCRAPE-ONLY MODE)
- **Files Created**:
  - `data/ecd_auto_design/urls.json` - 338 URLs discovered
  - `data/ecd_auto_design/scrape_html.py` - HTML scraper with retry logic
  - `data/ecd_auto_design/fix_urls.py` - URL correction script
- **Pipeline Status**: urlsFound=338, htmlScraped=1 (existing)
- **Critical Discovery**: URLs in showcase use `/portfolio-item/` prefix
  - Original discovery captured URLs as `/project-xyz/` (404 errors)
  - Fixed URLs to `/portfolio-item/project-xyz/` format
- **Challenge**: Sandbox environment blocks Python `requests` library
  - Standard scraping cannot make network requests
  - Requires: stealth_scraper.py or browser automation for Stage 2
- **Note**: Source removed from sources.json during session - may have been re-prioritized

**Learnings:**
1. Always verify URL patterns by testing actual page structure before scraping
2. Portfolio-style sites often use custom URL patterns (`/portfolio-item/` vs direct paths)
3. Sandbox restrictions require alternative scraping methods (MCP tools, stealth_scraper.py)

---

