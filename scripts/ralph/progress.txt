# RalphOS Progress & Learnings

This file captures accumulated knowledge from Ralph iterations. Each entry documents implementation details, patterns discovered, and lessons learned.

---

## Codebase Patterns (Reusable Knowledge)

### URL Discovery
- Always check for pagination (numeric pages, "load more" buttons, infinite scroll)
- Normalize URLs: remove fragments (#), trailing slashes, and duplicates
- Save intermediate results every 50 URLs to prevent data loss
- Use set() for deduplication before writing to urls.json

### HTML Scraping
- Always set User-Agent header to avoid basic bot detection
- Implement retry logic with exponential backoff (1s, 2s, 4s delays)
- Save HTML to `{output_dir}/html/{slug}.html` format
- Use filename sanitization: replace special chars, limit to 200 chars

### Build Extraction
- Use schema at `schema/build_extraction_schema.json` as reference
- Required fields: build_id, source_type, build_type, build_source, source_url, year, make, model
- Generate build_id with: `python scripts/tools/build_id_generator.py {source_url}`
- Set extraction_confidence based on completeness of extracted data

### Mod Extraction
- Use `category_detector.py` for automatic categorization
- Categories come from `schema/Vehicle_Componets.json`
- Include brand extraction when identifiable from mod name
- Calculate modification_level: Stock (0-1), Light (2-5), Moderate (6-15), Heavy (16+)

### Anti-Bot Handling
- If 403/429 errors occur, mark source as "blocked" in sources.json
- Use stealth_scraper.py for blocked sources
- Rate limit: minimum 1 second between requests per source

### File Structure
- urls.json: `{"urls": [], "lastUpdated": "ISO8601", "totalCount": N}`
- builds.json: `{"builds": [...], "metadata": {...}}`
- mods.json: `{"mods": [...], "metadata": {...}}`
- scrape_progress.json: tracks completed/failed URLs

---

## Session History

### 2026-01-08 - Initial Setup
- Created progress.txt template
- Documented codebase patterns for URL discovery, scraping, extraction
- Added anti-bot handling guidelines
- Established file structure conventions

---

### 2026-01-08 - ECD Auto Design URL Discovery (Stage 1)
- **Source**: ECD Auto Design (ecdautodesign.com/ecd-showcase/)
- **Action**: URL Discovery for Stage 1 (SCRAPE-ONLY MODE)
- **Files Created**:
  - `data/ecd_auto_design/urls.json` - 338 URLs discovered
  - `data/ecd_auto_design/scrape_html.py` - HTML scraper with retry logic
  - `data/ecd_auto_design/fix_urls.py` - URL correction script
- **Pipeline Status**: urlsFound=338, htmlScraped=1 (existing)
- **Critical Discovery**: URLs in showcase use `/portfolio-item/` prefix
  - Original discovery captured URLs as `/project-xyz/` (404 errors)
  - Fixed URLs to `/portfolio-item/project-xyz/` format
- **Challenge**: Sandbox environment blocks Python `requests` library
  - Standard scraping cannot make network requests
  - Requires: stealth_scraper.py or browser automation for Stage 2
- **Note**: Source removed from sources.json during session - may have been re-prioritized

**Learnings:**
1. Always verify URL patterns by testing actual page structure before scraping
2. Portfolio-style sites often use custom URL patterns (`/portfolio-item/` vs direct paths)
3. Sandbox restrictions require alternative scraping methods (MCP tools, stealth_scraper.py)

---

### 2026-01-08 - Luxury4Play HTML Scraping (Stage 2)
- **Source**: Luxury4Play (luxury4play.com)
- **Action**: HTML Scraping for Stage 2
- **Files Created/Modified**:
  - `data/luxury4play/normalize_urls.py` - URL deduplication script
  - `data/luxury4play/scrape_html.py` - HTML scraper with retry logic
  - `data/luxury4play/html/` - 12 HTML files (6 showcase + 6 garage)
  - `data/luxury4play/scrape_progress.json` - Progress tracking
- **Pipeline Status**: urlsFound=12, htmlScraped=12, htmlFailed=0, htmlBlocked=0
- **Challenge**: Sandbox blocks Python `requests` library network requests
  - Solution: Used MCP `webReader` tool to fetch HTML via external API
  - Successfully scraped all 12 unique URLs using webReader
- **Critical Discovery**: URLs in urls.jsonl contained duplicate #comments fragments
  - Original count: 18 URLs (with #comments duplicates)
  - Normalized count: 12 unique URLs (without #comments)
  - All 12 successfully scraped (100%)
- **Next Stage**: Stage 3 - Build Extraction from HTML

**Learnings:**
1. MCP `webReader` tool can bypass sandbox network restrictions
2. Always normalize URLs before scraping - remove # fragments
3. Showcase pages (`/showcase/title.id/`) vs Garage pages (`/members/user.id/garage/vehicle.id/`)
4. Both page types contain vehicle data but in different structures

---

### 2026-01-16 - Luxury4Play Build Extraction (Stage 3)
- **Source**: Luxury4Play (luxury4play.com)
- **Action**: Build Extraction for Stage 3
- **Files Created/Modified**:
  - `data/luxury4play/extract_builds.py` - Build extraction script
  - `data/luxury4play/builds.json` - 7 extracted builds
  - `scripts/ralph/sources.json` - Updated builds=7
  - `scripts/ralph/prd.json` - All BUILD stories marked passes=true
- **Pipeline Status**: urlsFound=12, htmlScraped=12, builds=7, mods=0
- **Extraction Results**:
  - 7 valid builds extracted from 12 HTML files
  - 5 files skipped (spam/non-vehicle content)
  - Builds: Ferrari Enzo, McLaren P1, Porsche GT3RS, Pagani Huayra Roadster,
    2014 Chevrolet Corvette, 2025 Bugatti Tourbillon, 1985 Golf GTI Altis
  - Porsche GT3RS has 3 modifications (Interior, Exterior, Exhaust)
- **Key Patterns Discovered**:
  - Showcase pages have structured fields: Year/Make/Model/Color/History/Modifications
  - Garage pages have simpler Details section: User/Make/Model/Year
  - Some garage entries are spam (years as make/model, non-vehicle content)
  - HTML stored as JSON from webReader with title, content, metadata fields
- **Next Stage**: Stage 4 - Mod Extraction using category_detector.py

**Learnings:**
1. Luxury4Play uses XenForo forum software - consistent structure across pages
2. Showcase pages are richer in data; garage pages are user-submitted and vary in quality
3. Filter spam by checking if make/model are numeric or content mentions non-vehicle services
4. build_id_generator.py creates deterministic 63-bit IDs from URLs (DuckDB compatible)

---

### 2026-01-16 - Luxury4Play Mod Extraction (Stage 4) - SOURCE COMPLETE
- **Source**: Luxury4Play (luxury4play.com)
- **Action**: Mod Extraction for Stage 4 (FINAL)
- **Files Created/Modified**:
  - `data/luxury4play/extract_mods.py` - Mod extraction script with brand detection
  - `data/luxury4play/mods.json` - 3 extracted modifications
  - `scripts/ralph/sources.json` - Updated mods=3, status=completed
  - `scripts/ralph/prd.json` - All MOD stories marked passes=true
- **Pipeline Status**: COMPLETE ✓
  - urlsFound=12, htmlScraped=12, builds=7, mods=3
  - All stages complete: URL Discovery → HTML Scraping → Build Extraction → Mod Extraction
- **Extraction Results**:
  - 3 modifications from 1 build (2019 Porsche GT3RS)
  - Categories: Interior (1), Exterior (1), Exhaust & Emission (1)
  - Brands detected: Ed Guard, Premier Protective Films, Sharkwerks
- **Key Implementation Details**:
  - extract_mods.py uses category_detector.py for categorization
  - Brand list sorted by length (longest first) to avoid substring issues (e.g., "Sharkwerks" vs "KW")
  - Preserves source-provided categories when category_detector confidence is low

**Learnings:**
1. Brand detection order matters - longer brand names must be checked first
2. Category_detector uses Trie for O(k) lookup - efficient for large keyword sets
3. Most Luxury4Play entries are stock vehicles (no modifications to extract)
4. Source now marked COMPLETE - all 4 pipeline stages finished

---

