{
  "projectName": "Roush Performance HTML Scraping",
  "sourceId": "roushperformance",
  "branchName": "main",
  "targetUrl": "https://roushperformance.com",
  "outputDir": "data/roushperformance",
  "pipelineStage": 2,
  "userStories": [
    {
      "id": "HTML-001",
      "title": "Create HTML scraper script with checkpoint support",
      "acceptanceCriteria": [
        "Create scrape_html.py with retry logic and exponential backoff",
        "Handle brotli/gzip Content-Encoding properly",
        "Save HTML to {outputDir}/html/{build_id}.html format",
        "Track progress in scrape_checkpoint.json and scrape_progress.jsonl"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Created scrape_html.py adapted from Hennessey template. Added proxy env var clearing."
    },
    {
      "id": "HTML-002",
      "title": "Execute HTML scraping for all URLs",
      "acceptanceCriteria": [
        "Scrape all 59 URLs discovered",
        "Handle 404s gracefully - mark as htmlFailed",
        "If blocked (403/429) - STOP and mark source as blocked"
      ],
      "priority": 2,
      "passes": true,
      "notes": "SUCCESS: 59/59 URLs scraped (100%). Zero failures, zero blocks. Shopify platform is scraper-friendly."
    },
    {
      "id": "HTML-003",
      "title": "Verify scraping completion and update sources.json",
      "acceptanceCriteria": [
        "Count HTML files in html/ directory",
        "Update sources.json pipeline: htmlScraped, htmlFailed, htmlBlocked",
        "Mark source in_progress or blocked based on results"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Verified 59 HTML files. Updated sources.json: status=in_progress, htmlScraped=59. Ready for Stage 3."
    }
  ],
  "createdAt": "2026-01-17T01:05:00Z",
  "createdBy": "ralph",
  "completedAt": "2026-01-17T01:20:00Z"
}
