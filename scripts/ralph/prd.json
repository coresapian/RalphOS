{
  "projectName": "Hennessey Performance HTML Scraping",
  "sourceId": "hennesseyperformance",
  "branchName": "main",
  "targetUrl": "https://hennesseyperformance.com",
  "outputDir": "data/hennesseyperformance",
  "pipelineStage": 2,
  "userStories": [
    {
      "id": "HTML-001",
      "title": "Create HTML scraper script with checkpoint support",
      "acceptanceCriteria": [
        "Create scrape_html.py with retry logic and exponential backoff",
        "Handle brotli/gzip Content-Encoding properly",
        "Save HTML to {outputDir}/html/{build_id}.html format",
        "Track progress in scrape_checkpoint.json and scrape_progress.jsonl"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Created scrape_html.py with proper compression handling. Rate limit: 2.5-5 seconds between requests."
    },
    {
      "id": "HTML-002",
      "title": "Execute HTML scraping for all URLs",
      "acceptanceCriteria": [
        "Scrape all 141 URLs discovered",
        "Handle 404s gracefully - mark as htmlFailed",
        "If blocked (403/429) - STOP and mark source as blocked"
      ],
      "priority": 2,
      "passes": true,
      "notes": "SUCCESS: 141/141 URLs scraped (100%). Zero failures, zero blocks. Site is scraper-friendly."
    },
    {
      "id": "HTML-003",
      "title": "Verify scraping completion and update sources.json",
      "acceptanceCriteria": [
        "Count HTML files in html/ directory",
        "Update sources.json pipeline: htmlScraped, htmlFailed, htmlBlocked",
        "Mark source complete or blocked based on results"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Verified 141 HTML files. Updated sources.json: status=in_progress, htmlScraped=141. Ready for Stage 3."
    }
  ],
  "createdAt": "2026-01-16T17:35:00Z",
  "createdBy": "ralph",
  "completedAt": "2026-01-16T17:42:00Z"
}
