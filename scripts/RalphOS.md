Specification for approval
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   RalphOS Codebase Analysis & Improvement Plan

   Executive Summary

   RalphOS is an autonomous AI agent loop system that executes multi-step web scraping tasks
   without human intervention. It wraps Claude CLI in a bash loop to process vehicle build data
   from multiple sources through a 4-stage pipeline.

   Current Status:
   â€¢  Total Cost Involved: 295 URLs discovered, 3 HTML scraped (1% progress)
   â€¢  5 sources pending in queue
   â€¢  Core infrastructure functional but several improvements identified

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Architecture Overview

   System Design

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                   ralph.sh (Bash Loop)                  â”‚
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
     â”‚  â”‚  Iteration Manager                                 â”‚  â”‚
     â”‚  â”‚  - Calls Claude CLI                                â”‚  â”‚
     â”‚  â”‚  - Monitors progress (10s interval)                 â”‚  â”‚
     â”‚  â”‚  - Commits changes                                 â”‚  â”‚
     â”‚  â”‚  - Updates PRD & sources.json                       â”‚  â”‚
     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚              Pipeline Stages (Sub-Ralph Agents)           â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚ URL Detectiveâ”‚ HTML Scraper â”‚ Build Extractorâ”‚Mod Extractorâ”‚
     â”‚ (Stage 1)   â”‚ (Stage 2)     â”‚ (Stage 3)     â”‚ (Stage 4)   â”‚
     â”‚ URL Discoveryâ”‚ HTML Fetch     â”‚ Data Extract  â”‚ Mod Categorizeâ”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                  Data Pipeline                           â”‚
     â”‚  urls.jsonl â†’ html/ â†’ builds.json â†’ mods.json         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Key Components

   Component              â”‚ Purpose                                    â”‚ Status          
   -----------------------+--------------------------------------------+-----------------
   `ralph.sh`             â”‚ Main orchestration loop                    â”‚ âœ… Functional
   `sources.json`         â”‚ Multi-project queue with pipeline tracking â”‚ âœ… Working
   `prompt.md`            â”‚ Agent behavior instructions                â”‚ âœ… Comprehensive
   `url-detective/`       â”‚ Stage 1: URL discovery agent               â”‚ âœ… Implemented
   `html-scraper/`        â”‚ Stage 2: HTML scraping agent               â”‚ âœ… Implemented
   `build-extractor/`     â”‚ Stage 3: Build data extraction             â”‚ âœ… Ready
   `mod-extractor/`       â”‚ Stage 4: Mod extraction & categorization   â”‚ âœ… Ready
   `stealth_scraper.py`   â”‚ Anti-bot scraping (Camoufox)               â”‚ âœ… Available
   `category_detector.py` â”‚ Automatic mod categorization               â”‚ âœ… Sophisticated
   `check_completion.sh`  â”‚ Status validation & correction             â”‚ âœ… Robust

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Core Components Analysis

   1. Main Orchestration (`scripts/ralph/ralph.sh`)

   Strengths:
   â€¢  âœ… Well-structured bash loop with proper signal handling
   â€¢  âœ… Progress monitoring every 10 seconds
   â€¢  âœ… Automatic archiving of completed projects
   â€¢  âœ… Color-rich terminal output for readability
   â€¢  âœ… Proper cleanup of child processes on SIGINT
   â€¢  âœ… Completion status validation before proceeding

   Issues Identified:
   1. No auto-resume capability - On restart, must manually track where to resume
   2. Limited error recovery - If Claude CLI crashes, loop continues without recovery logic
   3. No checkpoint system - Progress only saved after full iteration completion
   4. Hardcoded feature flags - SCRAPE_ONLY and VERBOSE could be in config
   5. No rate limiting awareness - Doesn't respect time delays between sources

   Recommendations:
   â€¢  Add checkpoint system (save iteration state to JSON file every 30s)
   â€¢  Implement auto-resume from last checkpoint on restart
   â€¢  Add exponential backoff for Claude CLI failures
   â€¢  Move feature flags to config file (scripts/ralph/config.json)

   2. Pipeline Tracking (`scripts/ralph/sources.json`)

   Strengths:
   â€¢  âœ… Comprehensive pipeline stage tracking (7 fields)
   â€¢  âœ… Status validation in check_completion.sh
   â€¢  âœ… Block event tracking (timestamp, type, count)
   â€¢  âœ… Multi-source queue management

   Issues Identified:
   1. No progress percentage tracking - Only raw counts, no visual progress bar
   2. Manual status updates required - No auto-sync from disk during runtime
   3. No source priority system - All pending sources processed linearly
   4. Missing retry counters - Track how many times blocked sources retried
   5. No cost tracking - No API call costs or time per source

   Recommendations:
   â€¢  Add priority field to sources (1-10, default 5)
   â€¢  Add attempted counter for blocked sources
   â€¢  Add lastAttempted timestamp
   â€¢  Add totalTimeSpent per source
   â€¢  Implement auto-source selection based on priority + status

   3. Sub-Stage Coordination

   Current Approach:
   â€¢  Each stage is an independent agent with own prompt.md
   â€¢  Ralph coordinates by updating sources.json pipeline fields
   â€¢  No direct inter-stage communication

   Issues Identified:

   2. No handoff protocol - No validation between stages
   3. Duplicate code - Similar URL discovery logic in multiple sources
   4. No test suite - Each stage script is manually tested

   Recommendations:
   â€¢  Implement stage transition validation script
   â€¢  Add unit tests for each stage
   â€¢  Create template scripts for new sources

   4. Data Schema & Extraction

   Schema Strengths:
   â€¢  âœ… Comprehensive vehicle build fields (30+ properties)
   â€¢  âœ… Strict typing with enums where appropriate
   â€¢  âœ… Conditional fields (sale_data, wheel_specs)
   â€¢  âœ… Validation rules (VIN pattern, year format)
   â€¢  âœ… Modification level auto-calculation (Stock/Heavy)

   Issues Identified:
   1. Missing fields for common data - No mileage for project builds
   2. No validation script - Schema defined but not enforced
   3. Versioning missing - Schema changes could break pipeline

   Recommendations:
   â€¢  Add schema version field ("version": "1.2.0")
   â€¢  Create validation script (validate_builds.py)
   â€¢  Add migration system for schema changes
   â€¢  Add markdown documentation for required vs optional fields

   5. Category Detector (`scripts/tools/category_detector.py`)

   Strengths:
   â€¢  âœ… Sophisticated fuzzy matching
   â€¢  âœ… Priority-based keyword lookup
   â€¢  âœ… Support for batch processing
   â€¢  âœ… Comprehensive component database
   â€¢  âœ… Confidence scoring

   Issues Identified:
   1. No learning from misclassifications - Manual corrections not saved
   2. Large component list - 500+ items, slower lookups
   3. No brand extraction - Could auto-detect brands from mod names
   4. Hard-coded categories - Cannot add new categories without code changes

   Recommendations:
   â€¢  Add category_corrections.json for learning from errors
   â€¢  Implement trie data structure for faster lookups
   â€¢  Add brand detector using known manufacturer list
   â€¢  Make categories configurable via JSON file

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Current State Assessment

   Pipeline Health

   Source              â”‚ Status      â”‚ URLs  â”‚ HTML â”‚ Builds â”‚ Mods â”‚ Progress
   --------------------+-------------+-------+------+--------+------+---------
   total_cost_involved â”‚ in_progress â”‚ 295   â”‚ 3    â”‚ null   â”‚ null â”‚ 1%
   custom_wheel_offset â”‚ blocked     â”‚ 4,497 â”‚ 69   â”‚ null   â”‚ null â”‚ 1%
   onallcylinders      â”‚ in_progress â”‚ ?     â”‚ ?    â”‚ null   â”‚ null â”‚ ?
   All Others          â”‚ pending     â”‚ 0     â”‚ 0    â”‚ null   â”‚ null â”‚ 0%

   Blocked Sources Analysis

   â€¢  custom_wheel_offset: 4,497 URLs, 69 scraped, 4,428 blocked by 403
     â€¢  Recommendation: Run stealth_scraper.py --source custom_wheel_offset

   â€¢  Potential blockers: Sources with aggressive anti-bot protection may all fail without
      Camoufox

   Total Cost Involved Specifics

   URL Discovery (Stage 1): âœ… Complete
   â€¢  295 URLs discovered
   â€¢  All follow pattern: https://totalcostinvolved.com/testimonials/{slug}/

   HTML Scraping (Stage 2): ğŸ”„ In Progress (1%)
   â€¢  Only 3 of 295 HTML files scraped
   â€¢  Issue: Scraping script exists but may have errors or be incomplete

   Root Cause Analysis:
   Looking at data/total_cost_involved/, multiple scraper scripts exist:
   â€¢  scrape_html.py (7,568 bytes)
   â€¢  scrape_html_mcp.py (8,314 bytes)
   â€¢  scrape_html_with_mcp.py (3,067 bytes)
   â€¢  batch_scraper.py (1,317 bytes)

   Multiple scraper iterations suggest:
   1. Initial approach failed â†’ created new script
   2. MCP approach attempted â†’ created MCP version
   3. No convergence on working solution

   Diagnosis Needed:

   bash
     python scripts/tools/diagnose_scraper.py data/total_cost_involved/

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Recommended Improvements

   Priority 1: Critical (Complete Blocking Issues)

   1. Fix Total Cost Involved Scraping - 1% progress suggests script bug
     â€¢  Run diagnostic tool
     â€¢  Check for timeout errors, blocking, or parsing issues
     â€¢  Verify URL structure matches scraper expectations
     â€¢  Estimated: 2-4 hours

   2. Implement Retry Logic for Blocked Sources
     â€¢  After stealth scraper success, auto-retry remaining URLs
     â€¢  Don't require manual intervention for every blocked source
     â€¢  Estimated: 4-6 hours

   3. Add Real-Time Progress Dashboard
     â€¢  Browser-based monitoring (already has dashboard_server.py)
     â€¢  Show pipeline status, errors, ETA
     â€¢  Estimated: 6-8 hours

   Priority 2: High (Improve Reliability)

   4. Add Checkpoint & Resume System
     â€¢  Save iteration state every 30 seconds
     â€¢  Auto-resume on script restart
     â€¢  Prevents re-running completed iterations
     â€¢  Estimated: 8-12 hours

   5. Implement Auto-Source Selection
     â€¢  Add priority field to sources.json
     â€¢  Pick highest priority pending source
     â€¢  Skip blocked sources unless manually specified
     â€¢  Estimated: 4-6 hours

   6. Add Stage Validation Tests
     â€¢  Unit tests for each stage script
     â€¢  Integration tests for full pipeline
     â€¢  Prevent regressions when adding new sources
     â€¢  Estimated: 12-16 hours

   Priority 3: Medium (Optimize Performance)

   7. Optimize Category Detector
     â€¢  Replace linear search with trie data structure
     â€¢  Expected speedup: 10-100x for large batches
     â€¢  Estimated: 6-8 hours

   8. Add Parallel Scraping Support
     â€¢  Scrape multiple URLs concurrently
     â€¢  Rate limit per source, not globally
     â€¢  Estimated 3-5x speedup for large sources
     â€¢  Estimated: 16-20 hours

   9. Create Source Templates
     â€¢  Template scripts for common site types (WordPress, custom gallery, etc.)
     â€¢  Reduce code duplication
     â€¢  Faster onboarding for new sources
     â€¢  Estimated: 8-12 hours

   Priority 4: Low (Quality of Life)

   10. Add Configuration System
     â€¢  Move hardcoded values to config file
     â€¢  Support per-source overrides
     â€¢  Estimated: 4-6 hours

   11. Improve Logging & Debugging
     â€¢  Structured logging (JSON format)
     â€¢  Log rotation (prevent massive log files)
     â€¢  Debug mode toggle
     â€¢  Estimated: 4-6 hours

   12. Add CLI Tool for Common Tasks
     â€¢  ralph-cli status - Show pipeline health
     â€¢  ralph-cli retry <source> - Retry blocked source
     â€¢  ralph-cli add <url> - Add new source to queue
     â€¢  Estimated: 8-10 hours

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Technical Debt & Code Quality Issues

   1. Bash Scripting (`ralph.sh`)

   Issues:
   â€¢  900+ line bash script (hard to maintain)
   â€¢  Heavy use of jq for JSON manipulation (slow)
   â€¢  No error handling for subprocess failures
   â€¢  Inline color codes (should use functions)

   Recommendation:
   â€¢  Refactor critical sections to Python
   â€¢  Use Python's json library instead of jq
   â€¢  Add try/except blocks for subprocess calls
   â€¢  Create color utility functions

   2. Progress Tracking

   Issues:
   â€¢  progress.txt doesn't exist (referenced in docs but missing)
   â€¢  No accumulated learnings documented
   â€¢  Manual updates required
   â€¢  No searchable knowledge base

   Recommendation:
   â€¢  Auto-create progress.txt if missing
   â€¢  Extract learnings from iteration logs automatically
   â€¢  Use Markdown with tags for searching
   â€¢  Implement knowledge base queries

   3. Data Storage

   Issues:
   â€¢  Multiple file formats (.json, .jsonl, .json)
   â€¢  No clear schema versioning
   â€¢  Mixed file locations (data/ vs scripts/ralph-stages/)
   â€¢  No backup/restore mechanism

   Recommendation:
   â€¢  Standardize on .jsonl for lists (one record per line)
   â€¢  Add schema version to all JSON files
   â€¢  Consolidate data storage under data/
   â€¢  Implement git-based backups before major operations

   4. Stealth Scraper (`stealth_scraper.py`)

   Issues:
   â€¢  600+ line monolithic script
   â€¢  No unit tests
   â€¢  Hardcoded delay values
   â€¢  No proxy rotation (mentioned in comments but not implemented)

   Recommendation:
   â€¢  Split into classes (StealthConfig, StealthScraper, ProxyManager)
   â€¢  Add unit tests with mocked browser
   â€¢  Make delays configurable per source
   â€¢  Implement proxy rotation pool

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Performance Optimization Opportunities

   1. Scraping Speed

   Current: ~1 URL per second (sequential)

   Optimizations:
   1. Concurrent requests: Use asyncio with semaphore (limit 10 concurrent)
   2. Connection pooling: Reuse HTTP connections
   3. Request batching: Fetch multiple pages in parallel

   Expected Speedup: 5-10x

   2. Data Processing

   Current: O(n) linear search for category detection

   Optimizations:
   1. Trie data structure: O(k) where k = keyword length
   2. Caching: Memoize frequently seen mod names
   3. Pre-computation: Build category lookup table on startup

   Expected Speedup: 50-100x for category detection

   3. I/O Bottlenecks

   Current: Synchronous file writes, no buffering

   Optimizations:
   1. Buffered writes: Flush to disk every 100 records
   2. Memory mapping: Use mmap for large JSONL files
   3. Compression: Compress old HTML files (gzip)

   Expected Speedup: 2-3x for file I/O

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Security & Reliability Concerns

   1. API Keys

   Issue: ZAI_API_KEY stored in .env (checked into git?)

   Recommendation:
   â€¢  Remove .env from git (already in .gitignore)
   â€¢  Add .env.example template
   â€¢  Document API key setup in README
   â€¢  Consider secret management (e.g., HashiCorp Vault)

   2. Anti-Bot Detection

   Current Approach:
   â€¢  Simple headers in regular scrapers
   â€¢  Camoufox for stealth scraping

   Issues:
   â€¢  No automatic fallback to Camoufox
   â€¢  No proxy rotation
   â€¢  No request rate limiting

   Recommendation:
   â€¢  Detect 403/429 errors and auto-switch to Camoufox
   â€¢  Implement request rate limiter per source
   â€¢  Add proxy rotation with health checks
   â€¢  Track success rates per scraping method

   3. Data Integrity

   Issues:
   â€¢  No checksums for downloaded files
   â€¢  No validation of extracted data
   â€¢  No rollback on extraction failures

   Recommendation:
   â€¢  Add SHA256 checksums for HTML files
   â€¢  ralph should Run validation script after each stage
   â€¢  Implement atomic writes (write to temp, then rename)
   â€¢  Add data restoration point before bulk operations

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Proposed Refactoring Roadmap

   Phase 1: Stabilization (Week 1)

   Goal: Complete blocked sources, fix critical bugs

   1. Fix Total Cost Involved scraping (Priority 1.1)
   2. Implement retry logic for stealth scrapes (Priority 1.2)
   3. Add checkpoint/resume system (Priority 2.1)

   Phase 2: Reliability (Week 2)

   Goal: Reduce manual intervention, improve uptime

   4. Implement auto-source selection (Priority 2.2)
   5. Add stage validation tests (Priority 2.3)
   6. Optimize category detector (Priority 3.1)

   7. Add parallel scraping (Priority 3.2)
   8. Create source templates (Priority 3.3)
   9. Refactor ralph.sh to Python (Priority - Debt 1)

   10. Add configuration system (Priority 4.1)
   11. Improve logging (Priority 4.2)
   12. Add CLI tool (Priority 4.3)
